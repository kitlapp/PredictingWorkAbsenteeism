{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efc4a91-1f30-4b9d-b54b-9cbdc64a2d7b",
   "metadata": {},
   "source": [
    "# Logistic Regression Model for Human Resources Management "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a72cb-eb76-40d8-9c64-a9d4bd7eccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.executable  # Display the path to the Python executable ensuring the correct env\"\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eedf6c-061f-4239-a46d-ac0817c6bf81",
   "metadata": {},
   "source": [
    "# Import Libraries and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e6ad7-996b-4851-ae04-d659c2f04445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # For numerical operations and arrays.\t\n",
    "import pandas as pd  # For data manipulation and analysis.\t\n",
    "import matplotlib.pyplot as plt  # For basic plotting.\t\n",
    "import seaborn as sns  # For enhanced plotting.\t\n",
    "from sklearn.preprocessing import StandardScaler  # For creating scaler instances for standardization purposes.\n",
    "from sklearn.model_selection import train_test_split  # For splitting the data into sets avoiding overfitting.\n",
    "from sklearn.linear_model import LogisticRegression  # For creating LogisticRegression instances.\n",
    "from sklearn import metrics  # For evaluating the model\n",
    "from python_scripts import summary_metrics\n",
    "from sklearn.model_selection import GridSearchCV  # For searching the best parameters over specified parameter values\n",
    "import joblib  # For saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137613c-3307-412b-9fa6-a3e1c1e1d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV Datafile to a DataFrame:\n",
    "df = pd.read_csv('cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81585794-84b5-4f9c-b6d7-0e486cabef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b6d2e4-513e-4d9b-b802-9661d127a8dd",
   "metadata": {},
   "source": [
    "# Create Features and Targets for Two Scaling Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ca69c-8252-4bc3-8e5e-7a687a8b5271",
   "metadata": {},
   "source": [
    "I 'll try two different scaling options and hence I 'll separate into two different feature DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f5064-f764-4a0d-b5f7-10bfb435655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoints:\n",
    "df_1 = df.copy()\n",
    "df_2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160600e7-fda4-4c38-9481-761e1e6f2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features_1 = df_1.iloc[:, :-1]  # All features except the target\n",
    "features_2 = df_2.iloc[:, :-1]  # All features except the target\n",
    "targets = df['Extensive Absenteeism Time in Hours']  # Common target for both versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53f046-10e8-4c41-84bc-b63c85fb0f34",
   "metadata": {},
   "source": [
    "# Shuffle and Split the Data for Two Scaling Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01867d-1a8e-4623-aff6-b1520b4d6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets for both versions:\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(\n",
    "    features_1, targets, test_size=0.15, random_state=7)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(\n",
    "    features_2, targets, test_size=0.15, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a738d-aa1b-4fff-83e3-5bfdcd222aee",
   "metadata": {},
   "source": [
    "# Scale Features (2 Versions are Provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0b41d-4800-40c4-8591-6ccc5b8b0f6f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "It is crucial to leave the dummy variables unscaled because they are already binary with values of 0 or 1. Additionally, leaving dummies unscaled enhances interpretability.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e8e75-66a3-415a-9f7b-5ec59648adde",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "We will try two different versions of scaling. In the first version, we will scale all features except for the dummy variables (df_1). In the second version, we will leave the date-related features unscaled, as they are discrete and not continuous variables, and we will scale only the remaining columns (df_2).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e74203-af48-4e02-8144-2c92ba18eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features to be scaled:\n",
    "scale_all_except_dummies = ['Month Absence Occurred', 'Monthday Range Absence Occurred', 'Weekday Absence Occurred', \n",
    "                            'Transportation Expense', 'Distance to Work', 'Age', 'Daily Work Load Average', \n",
    "                            'Body Mass Index']\n",
    "\n",
    "scale_not_all = ['Transportation Expense', 'Distance to Work', 'Age', 'Daily Work Load Average', 'Body Mass Index']\n",
    "\n",
    "# Create two scaler object for both versions:\n",
    "scaler_1 = StandardScaler()\n",
    "scaler_2 = StandardScaler()\n",
    "\n",
    "# Fit and Transform all features except for dummies for X_train_1:\n",
    "X_train_1[scale_all_except_dummies] = scaler_1.fit_transform(X_train_1[scale_all_except_dummies])\n",
    "\n",
    "# Transform X_test_1 using the same scaler\n",
    "X_test_1[scale_all_except_dummies] = scaler_1.transform(X_test_1[scale_all_except_dummies])\n",
    "\n",
    "# Fit and transform the features to scale (excluding dummies and date-related features) for X_train_2\n",
    "X_train_2[scale_not_all] = scaler_2.fit_transform(X_train_2[scale_not_all])\n",
    "\n",
    "# Transform X_test_2 using the same scaler\n",
    "X_test_2[scale_not_all] = scaler_2.transform(X_test_2[scale_not_all])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb58e3-990d-4a40-bc88-41df175bc5a5",
   "metadata": {},
   "source": [
    "# Baseline Model for Both Scaling Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb93d8-4370-484d-bbfe-04d1b50a8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logistic regression models for both versions\n",
    "model_1 = LogisticRegression()\n",
    "model_2 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36580dec-2c4c-48c1-a327-9dc673f7fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models with both versions of scaled data\n",
    "model_1.fit(X_train_1, y_train_1)\n",
    "model_2.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db045300-0baf-4823-94b5-5f7f8421cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function which summarizes the results of first version of scaling data:\n",
    "summary_df_1 = summary_metrics(feature_df=features_1, \n",
    "                             model=model_1, \n",
    "                             x_tr=X_train_1, \n",
    "                             y_tr=y_train_1, \n",
    "                             x_te=X_test_1, \n",
    "                             y_te=y_test_1)\n",
    "summary_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbbe5c2-2525-4f13-bf94-214d1e81adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function which summarizes the results of second version of scaling data:\n",
    "summary_df_2 = summary_metrics(feature_df=features_2, \n",
    "                             model=model_2, \n",
    "                             x_tr=X_train_2, \n",
    "                             y_tr=y_train_2, \n",
    "                             x_te=X_test_2, \n",
    "                             y_te=y_test_2)\n",
    "summary_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a1dd8-f5f1-4779-ac5b-caa4d23e0d17",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Based on the results observed, it appears that whether we scale all features except dummies or only some features does not significantly impact the model's performance. I manually tested the model with different random states, and the results consistently showed similar patterns. Additionally, in most cases, the weights of the date-related features are very close to zero. This suggests that we can safely drop these features to reduce dimensionality and choose any scaling option we prefer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715a2dc-7a48-4f6d-aa8c-1cfd935ad979",
   "metadata": {},
   "source": [
    "# Reducing Model Complexity (Model Version 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb24051-4670-4994-b763-449041f86cfc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Knowing that scaling or not scaling the date-related features doesn't affect the performance, we 'll choose to retain unscaled date-related features.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a1cfb-d1c0-493e-9863-e4d42bada1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec10b36-fb4b-44c8-af58-2e4fbe4061ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform backward elimination by dropping zero-weight features:\n",
    "df_3 = df_3.drop(columns=['Month Absence Occurred', 'Monthday Range Absence Occurred', 'Has 1 Child', 'Has 1 Pet',\n",
    "                                                          'Daily Work Load Average', 'Distance to Work'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b1d23-0f07-448f-9316-394b3d8bb422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract new features from the updated DataFrame:\n",
    "features_3 = df_3.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07dae7-d3cd-45e6-ba0f-f41fdbb23bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new train-test split based on the newly extracted features:\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(\n",
    "    features_3, targets, test_size=0.15, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe33f0-f9b7-4f06-8aaf-25784ac84027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features to be scaled:\n",
    "scale_not_all = ['Transportation Expense', 'Age', 'Body Mass Index']\n",
    "\n",
    "# Create two scaler object for both versions:\n",
    "scaler_3 = StandardScaler()\n",
    "\n",
    "# Fit and Transform X_train_3:\n",
    "X_train_3[scale_not_all] = scaler_3.fit_transform(X_train_3[scale_not_all])\n",
    "\n",
    "# Transform X_test_3 using the same scaler\n",
    "X_test_3[scale_not_all] = scaler_3.transform(X_test_3[scale_not_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56644e97-9e07-47f5-b129-d8c9a4e7ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new logistic regression model:\n",
    "model_3 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7123d-53c1-45a2-b3b4-d724bc9b1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the new model with the updated training set:\n",
    "model_3.fit(X_train_3, y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5117f372-d0b9-4dd0-8149-7b0970cfdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary metrics for the new model\n",
    "summary_df_3 = summary_metrics(feature_df=features_3, \n",
    "                             model=model_3, \n",
    "                             x_tr=X_train_3, \n",
    "                             y_tr=y_train_3, \n",
    "                             x_te=X_test_3, \n",
    "                             y_te=y_test_3)\n",
    "summary_df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7536d-d516-4a88-8732-1f4d828c4998",
   "metadata": {},
   "source": [
    "# Build a More Advanced Model (Model Version 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20642e-2070-4ccb-979f-5286a25d7438",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "We 'll try some other combinations to see if the model performance can be improved using GridSearchCV. We 'll use the simplified model from version 3. Therefore, X_train_3, X_test_3, y_train_3, and y_test_3 remain unchanged.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c25c71-35ea-4b9d-b5ca-fef9497edf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new logistic regression model:\n",
    "model_4 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eb8f29-f5c5-4dae-8c30-38c08b1d0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for different solvers without raising warnings:\n",
    "param_grid_liblinear = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000], # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],  # Regularization type\n",
    "    'solver': ['liblinear'],  # Optimization algorithm\n",
    "    'max_iter': [100, 200, 300, 400, 500]  # Maximum number of iterations for convergence\n",
    "}\n",
    "\n",
    "param_grid_newton_cg = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l2'],  \n",
    "    'solver': ['newton-cg'],\n",
    "    'max_iter': [100, 200, 300, 400, 500]  \n",
    "}\n",
    "\n",
    "param_grid_lbfgs = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l2'],  \n",
    "    'solver': ['lbfgs'],\n",
    "    'max_iter': [100, 200, 300, 400, 500] \n",
    "}\n",
    "\n",
    "param_grid_sag = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l2'],  \n",
    "    'solver': ['sag'],\n",
    "    'max_iter': [100, 200, 300, 400, 500]  \n",
    "}\n",
    "\n",
    "param_grid_saga = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  \n",
    "    'solver': ['saga'],\n",
    "    'max_iter': [100, 200, 300, 400, 500],\n",
    "    'l1_ratio': [0, 0.1, 0.5, 0.9, 1]  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f6ff5-cb2d-47a0-9d7d-33d95ce9f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup GridSearchCV changing only the param_grid parameter to meet all options of the cell above:\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_4,\n",
    "    param_grid=param_grid_liblinear,\n",
    "    scoring='accuracy',  # Evaluation metric\n",
    "    cv=5,  # Number of cross-validation folds\n",
    "    verbose=1,  # Verbosity level\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b411789d-0609-4f86-b77c-90c5e21fd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search with preprocessed data\n",
    "grid_search.fit(X_train_3, y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ac830-06ac-4ecb-86ab-7c397c1c4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the best parameters and scores\n",
    "print(\"Best Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"\\nBest Score:\")\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test data\n",
    "best_model = grid_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test_3, y_test_3)\n",
    "print(\"\\nTest Accuracy:\")\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee103c1e-aa37-4160-950a-b330c66db049",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "<strong>The results did not improve with different parameter grids for each solver. Therefore, we 'll keep the simplest model, that is, version 3.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8bf14-a575-435c-9f84-88984eff8186",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8ea2a-190d-4668-9f06-b773b30d0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames for saving the model and scaler:\n",
    "filename = 'model.joblib'\n",
    "scalername = 'scaler.joblib'\n",
    "\n",
    "# Save the logistic regression model:\n",
    "joblib.dump(model_3, filename)\n",
    "\n",
    "# Save the scaler used for data normalization:\n",
    "joblib.dump(scaler_3, scalername)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72309d-0b05-4854-8142-07e65c59cf8d",
   "metadata": {},
   "source": [
    "# Provide Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9ad1f-5c16-408f-909a-4f5081a18c7f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Some conclusions are intuitive. For example, 'Other Factor Absence' has the highest positive impact on the probability of absenteeism, which is expected as it indicates a significant health issue. Similarly, factors like 'Has More than 2 Children' and 'Has 2 Children' also have a positive impact on absenteeism, aligning with common expectations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac3950b-e395-4a81-8551-cf9cdf535b7c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Rather than focusing on these expected factors, it is more insightful to explore social variables. For instance, higher education levels are associated with a slight decrease in absenteeism odds. This may be due to increased job stability or motivation among more educated employees. Additionally, older employees show a lower probability of absenteeism, possibly due to greater experience and commitment to their jobs. An interesting finding is the negative impact of the 'Has More than 2 Pets' factor. While one might expect that owning multiple pets would lead to more absences due to veterinary visits or pet health issues, the data suggests otherwise. This could be because individuals with more than two pets likely have the support of a family to help care for them, reducing the impact on their own work attendance.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
